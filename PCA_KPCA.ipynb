{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for pca fit and transformation\n",
    "def pca_fit_transform(X, k = 1):\n",
    "    mean_matrix = np.mean(X, axis = 0)\n",
    "    X = X - mean_matrix              # centering data\n",
    "    covariance_matrix = np.cov(X.T)  # covariance matrix    \n",
    "    eigen_values, eigen_vectors= np.linalg.eig(covariance_matrix)   #eigenvalues and eigenvectors\n",
    "    eigen_pairs = [(eigen_values[i], eigen_vectors[:,i]) for i in range(eigen_values.shape[0])] # Making a list of (eigen_values, eigen_vectors) tuples\n",
    "    eigen_pairs.sort(key=lambda x: x[0], reverse=True)    # Sorting tuples in descending order eigen_values\n",
    "\n",
    "    # obtaining P with its column vectors corresponding to the top k eigenvectors\n",
    "    P = eigen_pairs[0][1].reshape(eigen_pairs[0][1].shape[0],1)\n",
    "    for i in range(1,k):\n",
    "        P = np.hstack((P,eigen_pairs[i][1].reshape(eigen_pairs[i][1].shape[0],1)))\n",
    "    transformed = X.dot(P)                       # transforming\n",
    "    return transformed, mean_matrix, P\n",
    "\n",
    "# method for pca transformation\n",
    "def pca_transform(X_test, mean_matrix, P):\n",
    "    X_test = X_test - mean_matrix                # centralizing data\n",
    "    transformed = X_test.dot(P)                  # transforming\n",
    "    return transformed\n",
    "\n",
    "# method for kernel pca fit and transformation\n",
    "def kpca_fit_transform(X, k = 1, kernel = 'linear'):\n",
    "    gamma = 0\n",
    "    N = X.shape[0]\n",
    "    if kernel == 'linear':\n",
    "        K = (X).dot(X.T)                                         # linear Kernel matrix\n",
    "        \n",
    "    elif kernel == 'rbf':\n",
    "        squared_euclidean = squareform(pdist(X, 'sqeuclidean'))  # squared Euclidean distance\n",
    "        square_form = squareform(squared_euclidean)              # square-form distance matrix to vector-form distance vector\n",
    "        variance = np.var(square_form)                           \n",
    "        gamma = 1/(2*variance)\n",
    "        K = np.exp(-gamma * squared_euclidean)                   # rbf Kernel matrix with gamma = 1/(2*variance)\n",
    "    \n",
    "    elif kernel == 'polynomial': \n",
    "        K = (X.dot(X.T) + 1) ** 2                                # polynomial kernel with c = 1 and d = 2\n",
    "        \n",
    "    # Centering K\n",
    "    one_matrix = np.ones((N,N)) / N\n",
    "    K = K - one_matrix.dot(K) - K.dot(one_matrix) + one_matrix.dot(K).dot(one_matrix)\n",
    "    \n",
    "    eigen_values, eigen_vectors = scipy.linalg.eigh(K)            # getting eigen values and eigen vectors\n",
    "    \n",
    "    eigen_vectors = eigen_vectors / (np.sqrt(eigen_values)*np.linalg.norm(eigen_vectors))   # normalizing eigen vectors\n",
    "    indexes = eigen_values.argsort()[::-1]                        # sorting\n",
    "\n",
    "    P = eigen_vectors[:, indexes[0: k]]                           # projection matrix\n",
    "    X_transformed = K.dot(P)                                      # transforming\n",
    "    return X_transformed, K, P, gamma\n",
    "\n",
    "# method for kernel pca transformation\n",
    "def kpca_transform(X_train, X_test, K_train, P, gamma=1, kernel = 'linear'):\n",
    "    # kernel matrix\n",
    "    if kernel == 'linear':\n",
    "        K = X_test.dot(X_train.T)\n",
    "\n",
    "    elif kernel == 'rbf':\n",
    "        stack = np.vstack((X_train, X_test))\n",
    "        distance = squareform(pdist(stack, 'sqeuclidean'))\n",
    "        distance[distance<0] = 0\n",
    "        distance =pd.DataFrame(distance)\n",
    "        distance = np.array(distance.iloc[X_train.shape[0]:, :X_train.shape[0]])\n",
    "        K = np.exp(-gamma * distance) \n",
    "\n",
    "    elif kernel == 'polynomial':\n",
    "         K = (X_test.dot(X_train.T) + 1) ** 2  \n",
    "    \n",
    "    # transforming\n",
    "    N  = X_train.shape[0]\n",
    "    Nt = X_test.shape[0]\n",
    "    N2 = N*N\n",
    "    one_test = np.ones((Nt,N))\n",
    "    one_train = np.ones((N,N))\n",
    "    s = (1/N2)*K.sum()\n",
    "\n",
    "    K = K - ((1/N)*(K.dot(one_train))) - ((1/N)*(one_test.dot(K_train))) + (((1/N2)*s)*(one_test.dot(one_train)))\n",
    "    \n",
    "    transformed = K.dot(P)\n",
    "    \n",
    "    return transformed                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 7291 samples and 256 features. \n",
      "Testing data has 2007 samples and 256 features.\n",
      "\n",
      "Model Accuracy before dimension reduction:  0.9501743896362731\n",
      "\n",
      "Reducing dimensions from  256  to  64\n",
      "\n",
      "Model Accuracy after PCA dimension reduction:  0.9501743896362731\n",
      "Model Accuracy after KPCA dimension reduction using  rbf  kernel:  0.924265072247135\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":   \n",
    "    # reading in data\n",
    "    train_data = pd.read_csv(\"Data/usps.csv\", header = None)\n",
    "    test_data = pd.read_csv(\"Data/usps.t.csv\", header = None)\n",
    "\n",
    "    # splitting data and labels\n",
    "    train_X = np.asarray(train_data.drop([0], axis = 1))\n",
    "    train_y = np.asarray(train_data[0])\n",
    "    test_X = np.asarray(test_data.drop([0], axis = 1))\n",
    "    test_y = np.asarray(test_data[0])\n",
    "\n",
    "    print('Training data has', train_X.shape[0],'samples and', train_X.shape[1],'features.',\n",
    "          '\\nTesting data has', test_X.shape[0],'samples and', test_X.shape[1],'features.')\n",
    "    \n",
    "    model = KNeighborsClassifier() #using KNN classifier for classification\n",
    "    model.fit(train_X,train_y)\n",
    "    print(\"\\nModel Accuracy before dimension reduction: \",model.score(test_X, test_y))\n",
    "    \n",
    "    \n",
    "    reduced_dim = 64 \n",
    "    print(\"\\nReducing dimensions from \",train_X.shape[1], \" to \",reduced_dim)\n",
    "    \n",
    "    # PCA\n",
    "    X_train_transformed, mean_mat, projection = pca_fit_transform(train_X, k = reduced_dim)\n",
    "    X_test_transformed = pca_transform(test_X, mean_mat, projection)\n",
    "    model.fit(X_train_transformed,train_y)\n",
    "    print(\"\\nModel Accuracy after PCA dimension reduction: \",model.score(X_test_transformed, test_y))\n",
    "\n",
    "    # KPCA\n",
    "    kernel_ = \"rbf\"     # kernel options: linear, rbf, polynomial\n",
    "    X_train_transformed, k_train, projection, gamma = kpca_fit_transform(train_X, k = reduced_dim, kernel = kernel_)\n",
    "    X_test_transformed = kpca_transform(train_X, test_X, k_train, projection, gamma=gamma, kernel = kernel_)\n",
    "    model.fit(X_train_transformed,train_y)\n",
    "    print(\"Model Accuracy after KPCA dimension reduction using \", kernel_,\" kernel: \",model.score(X_test_transformed, test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
